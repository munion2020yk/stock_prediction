import torch
import torch.nn as nn

# ======================================================
# 1. LSTM ëª¨ë¸ ì •ì˜ (ì´ì „ê³¼ ë™ì¼, ì™„ì„±í˜•)
# ======================================================

class LSTM(nn.Module):
    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):
        """
        num_layers íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§€ì •ëœ ìˆ˜ì˜ LSTM ì¸µì„ ìë™ìœ¼ë¡œ ìŒ“ìŠµë‹ˆë‹¤.
        """
        super(LSTM, self).__init__()
        self.num_classes = num_classes
        self.num_layers = num_layers
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.seq_length = seq_length

        # nn.LSTMì´ num_layersë¥¼ ì¸ìë¡œ ë°›ì•„ ìë™ìœ¼ë¡œ ì—¬ëŸ¬ ì¸µì„ ìŒ“ì•„ì¤ë‹ˆë‹¤.
        self.lstm = nn.LSTM(
            input_size=input_size, 
            hidden_size=hidden_size,
            num_layers=num_layers,      # (ì˜ˆ: 1, 2, 3...)
            batch_first=True
        )
        
        self.fc_1 = nn.Linear(hidden_size, 128)
        self.fc = nn.Linear(128, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        # x shape: (batch_size, seq_length, input_size)
        
        # (hn, cn) shape: (num_layers, batch, hidden_size)
        out, (hn, cn) = self.lstm(x)
        
        # ë§ˆì§€ë§‰ ì¸µ(layer)ì˜ ë§ˆì§€ë§‰ ì‹œì (time step) ì€ë‹‰ ìƒíƒœë§Œ ì‚¬ìš©
        hn_last_layer = hn[-1]  # (batch, hidden_size)
        
        # FC Layer í†µê³¼
        out = self.relu(hn_last_layer)
        out = self.fc_1(out)
        out = self.relu(out)
        out = self.fc(out)
        
        return out


# ======================================================
# 2. CNN + LSTM ëª¨ë¸ ì •ì˜ (â­ cnn_num_layers ì ìš©)
# ======================================================
class CNN_LSTM(nn.Module):
    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length,
                 cnn_num_layers=1, num_filters=64, kernel_size=3):
        """
        Args:
            num_layers (int): ìŒ“ì„ "LSTM" ì¸µì˜ ê°œìˆ˜
            cnn_num_layers (int): ìŒ“ì„ "CNN" ì¸µì˜ ê°œìˆ˜
        """
        super(CNN_LSTM, self).__init__()
        
        self.num_classes = num_classes
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers      # LSTM ì¸µ ìˆ˜
        self.seq_length = seq_length
        self.cnn_num_layers = cnn_num_layers # CNN ì¸µ ìˆ˜
        self.num_filters = num_filters

        # ğŸ”¹ 1. CNN Stack (ë™ì ìœ¼ë¡œ ìƒì„±)
        self.cnn_stack = nn.ModuleList()
        
        current_in_channels = input_size # ì²« ë²ˆì§¸ CNN ì¸µì˜ ì…ë ¥ ì±„ë„
        
        for i in range(cnn_num_layers):
            # Conv1d ì¶”ê°€
            self.cnn_stack.append(
                nn.Conv1d(
                    in_channels=current_in_channels,
                    out_channels=num_filters,
                    kernel_size=kernel_size,
                    padding=kernel_size // 2  # ì‹œí€€ìŠ¤ ê¸¸ì´ ìœ ì§€
                )
            )
            # BatchNorm1d ì¶”ê°€
            self.cnn_stack.append(nn.BatchNorm1d(num_filters))
            # ReLU ì¶”ê°€
            self.cnn_stack.append(nn.ReLU())
            
            # ë‹¤ìŒ CNN ì¸µì˜ ì…ë ¥ ì±„ë„ì€
            # í˜„ì¬ ì¸µì˜ ì¶œë ¥ ì±„ë„(num_filters)ì´ ë©ë‹ˆë‹¤.
            current_in_channels = num_filters 

        # ğŸ”¹ 2. LSTM Module
        # (ì¤‘ìš”) LSTMì˜ input_sizeëŠ” CNN ìŠ¤íƒì˜ ìµœì¢… ì¶œë ¥ ì±„ë„ ìˆ˜(num_filters)
        self.lstm_module = LSTM(
            num_classes=num_classes,
            input_size=num_filters,   # â­ï¸ CNNì˜ ìµœì¢… ì¶œë ¥ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ
            hidden_size=hidden_size,
            num_layers=num_layers,    # â­ï¸ LSTM ì¸µ ìˆ˜ë¥¼ ì—¬ê¸°ì— ì „ë‹¬
            seq_length=seq_length
        )

    def forward(self, x):
        # -----------------------------
        # ì…ë ¥ x: (batch, seq_len, input_size)
        # -----------------------------
        
        # 1. CNN ì…ë ¥ìš©ìœ¼ë¡œ ë³€í™˜: (batch, input_size, seq_len)
        x_cnn = x.permute(0, 2, 1)
        
        # 2. ë™ì ìœ¼ë¡œ ìƒì„±ëœ CNN ìŠ¤íƒ ëª¨ë‘ í†µê³¼
        for layer in self.cnn_stack:
            x_cnn = layer(x_cnn)
        # x_cnn shape: (batch, num_filters, seq_len)
        
        # 3. LSTM ì…ë ¥ìš©ìœ¼ë¡œ ë³€í™˜: (batch, seq_len, num_filters)
        x_lstm_in = x_cnn.permute(0, 2, 1)
        
        # 4. LSTM ëª¨ë“ˆ í†µê³¼ (LSTM + FC Layers)
        out = self.lstm_module(x_lstm_in)
        
        return out
    
    

class CNNModel(nn.Module):
    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length, cnn_num_layers=1, num_filters=64, kernel_size=3, output_size = 1):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv1d(input_size, num_filters, kernel_size, padding='same')
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool1d(2)
        pooled_len = seq_length // 2
        self.fc = nn.Linear(num_filters * pooled_len, output_size)
    def forward(self, x):
        x = x.permute(0, 2, 1)
        x = self.pool(self.relu(self.conv1(x)))
        return self.fc(x.flatten(1))
