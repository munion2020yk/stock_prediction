import streamlit as st
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import io
import time

# --- 1. ì„¤ì • ë° í´ë˜ìŠ¤ ì •ì˜ ---

st.set_page_config(page_title="Advanced Stock Prediction App", layout="wide")
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Scaler í´ë˜ìŠ¤
class CustomMinMaxScaler:
    def __init__(self):
        self.min_ = None
        self.max_ = None
        self.scale_ = None
    
    def fit_transform(self, data):
        self.min_ = np.min(data, axis=0)
        self.max_ = np.max(data, axis=0)
        self.scale_ = self.max_ - self.min_
        self.scale_[self.scale_ == 0] = 1.0
        return (data - self.min_) / self.scale_
    
    def transform(self, data):
        if self.min_ is None:
            raise RuntimeError("Scaler not fitted.")
        return (data - self.min_) / self.scale_
    
    def inverse_transform_col(self, data, col_index):
        return (data * self.scale_[col_index]) + self.min_[col_index]

# --- ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (Factory Pattern í™œìš©ì„ ìœ„í•´ í†µì¼ëœ ë¶€ëª¨ í´ë˜ìŠ¤ ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ë§Œ, ì—¬ê¸°ì„  ê°œë³„ ì •ì˜) ---

class BaseModel(nn.Module):
    def __init__(self):
        super().__init__()

class CNNModel(BaseModel):
    def __init__(self, input_dim, seq_len, output_dim):
        super().__init__()
        self.conv1 = nn.Conv1d(input_dim, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool1d(2)
        # CNN ì¶œë ¥ í¬ê¸° ê³„ì‚°: pool(L) -> L//2
        self.fc1 = nn.Linear(64 * (seq_len // 2), 50)
        self.fc2 = nn.Linear(50, output_dim)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.permute(0, 2, 1) # (N, C, L)
        x = self.pool(self.relu(self.conv1(x)))
        x = x.flatten(1)
        x = self.relu(self.fc1(x))
        return self.fc2(x)

class SimpleLSTM(BaseModel):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.relu = nn.ReLU()

    def forward(self, x):
        _, (h_n, _) = self.lstm(x)
        return self.fc(self.relu(h_n[-1]))

class CNN_LSTM(BaseModel):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.conv = nn.Conv1d(input_dim, 64, 3, padding=1)
        self.pool = nn.MaxPool1d(2)
        self.lstm = nn.LSTM(64, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.permute(0, 2, 1)
        x = self.pool(self.relu(self.conv(x)))
        x = x.permute(0, 2, 1) # LSTM ì…ë ¥ì„ ìœ„í•´ ë‹¤ì‹œ (N, L, C)ë¡œ ë³€í™˜
        _, (h_n, _) = self.lstm(x)
        return self.fc(h_n[-1])

class AttentionLSTM(BaseModel):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.attention = nn.Linear(hidden_dim * 2, 1)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)

    def forward(self, x):
        outputs, _ = self.lstm(x)
        score = self.attention(outputs)
        weights = F.softmax(score, dim=1)
        context = torch.sum(outputs * weights, dim=1)
        return self.fc(context)

# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
@st.cache_data
def load_data(uploaded_file):
    encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'utf-16']
    df = pd.DataFrame()
    bytes_data = uploaded_file.getvalue()
    
    for enc in encodings:
        try:
            df = pd.read_csv(io.BytesIO(bytes_data), sep='\t', index_col='Date', parse_dates=['Date'], encoding=enc)
            if len(df.columns) <= 1:
                df = pd.read_csv(io.BytesIO(bytes_data), sep=',', index_col='Date', parse_dates=['Date'], encoding=enc)
            break
        except:
            continue
            
    if not df.empty:
        df = df[df.index.notna()]
        df.sort_index(inplace=True)
        df.ffill(inplace=True)
        df.dropna(inplace=True)
    return df

# ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜
def create_sequences(data, time_step, horizon):
    X, y = [], []
    for i in range(len(data) - time_step - horizon + 1):
        X.append(data[i:(i + time_step), :])
        last_price = data[i + time_step - 1, 0] # 0ë²ˆ ì»¬ëŸ¼ = Target
        future_prices = data[i + time_step : i + time_step + horizon, 0]
        y.append(future_prices - last_price)
    return np.array(X), np.array(y)

# í•™ìŠµ í•¨ìˆ˜ (ê°œë³„ ëª¨ë¸ìš©)
def train_model(model, train_loader, epochs, progress_bar_slot, model_idx, total_models):
    model.to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()
    
    model.train()
    progress_bar = progress_bar_slot.progress(0, text=f"[{model_idx}/{total_models}] Training...")
    
    for epoch in range(epochs):
        avg_loss = 0
        for X, y in train_loader:
            X, y = X.to(DEVICE), y.to(DEVICE)
            optimizer.zero_grad()
            output = model(X)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()
            avg_loss += loss.item()
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ë„ˆë¬´ ìì£¼ëŠ” ìƒëµ ê°€ëŠ¥)
        if (epoch + 1) % 5 == 0 or epoch == 0:
            progress_bar.progress((epoch + 1) / epochs, text=f"[{model_idx}/{total_models}] Epoch {epoch+1}/{epochs} (Loss: {avg_loss/len(train_loader):.5f})")
    
    return model

# --- 2. Streamlit UI êµ¬ì„± ---

def main():
    st.title("ğŸ“ˆ Advanced Stock Prediction: Multi-Model Ensemble")
    st.markdown("""
    KOSPI ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ë©´ **5ê°€ì§€ ë‹¤ë¥¸ ëª¨ë¸**ë¡œ í•™ìŠµí•˜ê³ , 
    ê·¸ ê²°ê³¼ë¥¼ ì¢…í•©í•œ **ì•™ìƒë¸”(í‰ê· )** ì˜ˆì¸¡ê°’ì„ ì œê³µí•©ë‹ˆë‹¤.
    """)

    # ì‚¬ì´ë“œë°”: ì„¤ì •
    st.sidebar.header("1. ë°ì´í„° ë° ì„¤ì •")
    uploaded_file = st.sidebar.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ (KOSPI_base.csv)", type=['csv', 'txt'])

    if uploaded_file is not None:
        df = load_data(uploaded_file)
        
        if df.empty:
            st.error("íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        st.sidebar.success("ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
        
        # 2. Feature ì„ íƒ
        st.sidebar.subheader("2. Feature ì„ íƒ")
        all_columns = df.columns.tolist()
        target_col = 'KOSPI_Close'
        if target_col not in all_columns:
            st.error(f"'{target_col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        feature_options = [c for c in all_columns if c != target_col]
        selected_features = st.sidebar.multiselect(
            "ë³´ì¡° ì§€í‘œ ì„ íƒ",
            options=feature_options,
            default=feature_options[:min(4, len(feature_options))]
        )
        final_cols = [target_col] + selected_features
        
        st.sidebar.markdown("---")
        st.sidebar.subheader("3. ì˜ˆì¸¡ ì„¤ì •")
        
        min_date = df.index.min().date()
        max_date = df.index.max().date()
        default_pred_date = max_date + pd.Timedelta(days=1)
        if default_pred_date.weekday() >= 5:
             default_pred_date += pd.Timedelta(days=(7 - default_pred_date.weekday()))
             
        predict_start_date = st.sidebar.date_input(
            "ì˜ˆì¸¡ ì‹œì‘ ë‚ ì§œ",
            value=default_pred_date,
            min_value=min_date + pd.Timedelta(days=60)
        )
        
        epochs = st.sidebar.slider("ëª¨ë¸ë³„ í•™ìŠµ Epochs", 10, 100, 30) # ë‹¤ì¤‘ ëª¨ë¸ì´ë¯€ë¡œ ê¸°ë³¸ê°’ ë‚®ì¶¤
        time_step = 60
        horizon = 5

        st.subheader("ğŸ“Š ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        st.write(f"**Target:** {target_col} | **Features:** {selected_features}")
        st.dataframe(df[final_cols].tail(5))

        if st.button("ğŸš€ ì „ì²´ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ì‹œì‘", type="primary"):
            
            # --- ë°ì´í„° ì¤€ë¹„ ---
            cutoff_date = pd.to_datetime(predict_start_date) - pd.Timedelta(days=1)
            train_df = df.loc[:cutoff_date, final_cols]
            
            if len(train_df) < time_step + horizon:
                st.error("ë°ì´í„° ë¶€ì¡±.")
                return

            st.info(f"í•™ìŠµ ê¸°ê°„: {train_df.index.min().date()} ~ {train_df.index.max().date()}")

            scaler = CustomMinMaxScaler()
            train_data = train_df.values
            scaled_train_data = scaler.fit_transform(train_data)
            
            X, y = create_sequences(scaled_train_data, time_step, horizon)
            
            batch_size = 32
            dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32))
            train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
            
            # --- ëª¨ë¸ êµ¬ì„± ---
            input_dim = len(final_cols)
            
            # (ëª¨ë¸ëª…, í´ë˜ìŠ¤, íŒŒë¼ë¯¸í„°)
            models_config = [
                ("CNN", CNNModel, {"input_dim": input_dim, "seq_len": time_step, "output_dim": horizon}),
                ("CNN+LSTM", CNN_LSTM, {"input_dim": input_dim, "hidden_dim": 64, "output_dim": horizon}),
                ("LSTM(Basic)", SimpleLSTM, {"input_dim": input_dim, "hidden_dim": 64, "output_dim": horizon}),
                ("LSTM(Attention)", AttentionLSTM, {"input_dim": input_dim, "hidden_dim": 64, "output_dim": horizon}),
                ("LSTM(Deep)", SimpleLSTM, {"input_dim": input_dim, "hidden_dim": 128, "output_dim": horizon})
            ]
            
            predictions = {} # ê²°ê³¼ ì €ì¥ìš©
            
            # ì˜ˆì¸¡ìš© ì…ë ¥ ë°ì´í„° (ë§ˆì§€ë§‰ 60ì¼)
            last_sequence = scaled_train_data[-time_step:] 
            input_tensor = torch.tensor(last_sequence, dtype=torch.float32).unsqueeze(0).to(DEVICE)
            last_close_scaled = last_sequence[-1, 0]

            # --- í•™ìŠµ ë£¨í”„ ---
            total_models = len(models_config)
            cols = st.columns(total_models) # ì§„í–‰ë°”ë¥¼ ê°€ë¡œë¡œ ë°°ì¹˜í•˜ê±°ë‚˜
            
            main_progress = st.container() # ë©”ì¸ ì§„í–‰ ì˜ì—­
            
            for idx, (name, cls, params) in enumerate(models_config):
                with main_progress:
                    st.write(f"**{idx+1}. {name} ëª¨ë¸ í•™ìŠµ ì¤‘...**")
                    prog_bar = st.empty()
                    
                    # ëª¨ë¸ ì´ˆê¸°í™”
                    model = cls(**params)
                    
                    # í•™ìŠµ
                    model = train_model(model, train_loader, epochs, prog_bar, idx+1, total_models)
                    
                    # ì˜ˆì¸¡
                    model.eval()
                    with torch.no_grad():
                        pred_change = model(input_tensor).cpu().numpy().flatten()
                    
                    # ìŠ¤ì¼€ì¼ ë³µì›
                    pred_price_scaled = pred_change + last_close_scaled
                    pred_price = scaler.inverse_transform_col(pred_price_scaled, 0)
                    
                    predictions[name] = pred_price
                    st.toast(f"{name} í•™ìŠµ ì™„ë£Œ!")

            st.success("ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
            
            # --- ê²°ê³¼ ì§‘ê³„ ë° ì‹œê°í™” ---
            st.markdown("---")
            st.subheader(f"ğŸ“… ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ ({predict_start_date} ~)")
            
            pred_dates = pd.date_range(start=predict_start_date, periods=horizon, freq='B')
            
            # DataFrame ìƒì„±
            res_data = {"ë‚ ì§œ": pred_dates.strftime('%Y-%m-%d')}
            
            # ê° ëª¨ë¸ë³„ ì˜ˆì¸¡ê°’ ì¶”ê°€
            ensemble_preds = np.zeros(horizon)
            for name, pred in predictions.items():
                res_data[name] = np.round(pred, 2)
                ensemble_preds += pred
            
            # ì•™ìƒë¸” (í‰ê· ) ê³„ì‚°
            ensemble_preds /= total_models
            res_data["Ensemble (Avg)"] = np.round(ensemble_preds, 2)
            
            # ì‹¤ì œê°’ (ë°ì´í„°ê°€ ìˆë‹¤ë©´)
            full_df = df.loc[predict_start_date:, target_col]
            actual_vals = []
            for d in pred_dates:
                actual_vals.append(full_df.loc[d] if d in full_df.index else None)
            
            if any(v is not None for v in actual_vals):
                res_data["Actual"] = actual_vals

            result_df = pd.DataFrame(res_data)
            
            # í…Œì´ë¸” í‘œì‹œ (ì•™ìƒë¸” ê°•ì¡°)
            st.dataframe(result_df.style.highlight_max(axis=0, color='lightgreen', subset=["Ensemble (Avg)"]), use_container_width=True)
            
            # ê·¸ë˜í”„
            fig, ax = plt.subplots(figsize=(12, 6))
            
            # ê°œë³„ ëª¨ë¸ (ì ì„ , ì–‡ê²Œ)
            for name in predictions.keys():
                ax.plot(result_df['ë‚ ì§œ'], result_df[name], linestyle=':', alpha=0.7, label=name)
            
            # ì•™ìƒë¸” (ì‹¤ì„ , êµµê²Œ, ë¹¨ê°•)
            ax.plot(result_df['ë‚ ì§œ'], result_df["Ensemble (Avg)"], color='red', linewidth=3, marker='o', label='Ensemble (Avg)')
            
            # ì‹¤ì œê°’ (ê²€ì • ì‹¤ì„ )
            if "Actual" in result_df.columns and result_df["Actual"].notna().any():
                ax.plot(result_df['ë‚ ì§œ'], result_df["Actual"], color='black', linewidth=2, marker='s', label='Actual')

            ax.set_title("Multi-Model Prediction Comparison")
            ax.set_xlabel("Date")
            ax.set_ylabel("Price")
            ax.legend()
            ax.grid(True, alpha=0.3)
            plt.xticks(rotation=45)
            st.pyplot(fig)

    else:
        st.info("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
